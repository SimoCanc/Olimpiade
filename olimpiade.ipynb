{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":86023,"databundleVersionId":9869096,"sourceType":"competition"},{"sourceId":10855199,"sourceType":"datasetVersion","datasetId":6590916}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# https://huggingface.co/cutelemonlili/Qwen2.5-Math-1.5B-Instruct_MATH_training_response_AIDC-AI__Marco-o1_common_correct_macro_7b\n# https://www.kaggle.com/code/liondude/lb-20-qwq-32b-preview-optimized-inference#Inference\n# https://huggingface.co/docs/transformers/en/perf_infer_gpu_one\n# https://huggingface.co/Qwen/Qwen2.5-Math-1.5B-Instruct\n!pip install faiss-gpu sentence-transformers\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom sentence_transformers import SentenceTransformer\nimport pandas as pd\nimport numpy as np\nimport faiss\nimport torch\n\n\nimport re\nimport os\nimport polars as pl\nimport kaggle_evaluation.aimo_2_inference_server as inference_server\n\npd.set_option('display.max_colwidth', None)","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SEED = 123\n\nMODEL = \"Qwen/Qwen2.5-Math-1.5B-Instruct\"\nEMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\" \n\nTOP_K = 1\nTHRESHOLD = 0.25\n\nDEBUG = True","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if DEBUG:\n    df_reasoning = pd.read_csv('/kaggle/input/reasoning/reasoning.csv')[:-1]\n    \n    df_problems_text = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-progress-prize-2/reference.csv')[:-1]\n    \n    examples_df = pd.merge(df_problems_text, df_reasoning, on=\"answer\", how=\"inner\")\n    \n    del df_reasoning, df_problems_text\n    \n    \nelse:\n    df_reasoning = pd.read_csv('/kaggle/input/reasoning/reasoning.csv')\n    \n    df_problems_text = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-progress-prize-2/reference.csv')\n    \n    examples_df = pd.merge(df_problems_text, df_reasoning, on=\"answer\", how=\"inner\")\n    \n    del df_reasoning, df_problems_text\n    \n\n#examples_df   ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_model():\n    model = AutoModelForCausalLM.from_pretrained(MODEL, torch_dtype=torch.float16).to('cuda')\n    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n    return model, tokenizer\n\ndef find_similar_examples(current_problem: str, examples_df: pd.DataFrame, similarity_threshold=THRESHOLD):\n    if examples_df is None or examples_df.empty:\n        return pd.DataFrame()\n    \n    # Load sentence transformer model\n    embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n    # Generate embeddings for current problem and all examples\n    current_embedding = embedding_model.encode(current_problem.tolist(), convert_to_tensor=False)\n    example_embeddings = embedding_model.encode(examples_df['problem'].tolist(), convert_to_tensor=False)\n    \n    # Normalize embeddings for cosine similarity\n    current_embedding = current_embedding.astype(np.float32)\n    example_embeddings = example_embeddings.astype(np.float32)\n    faiss.normalize_L2(current_embedding)\n    faiss.normalize_L2(example_embeddings)\n    \n    # Build FAISS index\n    dimension = example_embeddings.shape[1]\n    index = faiss.IndexFlatIP(dimension)  # Inner product = cosine similarity for normalized vectors\n    index.add(example_embeddings)\n    \n    # Search for similar examples\n    similarities, indices = index.search(current_embedding, TOP_K)\n\n    print('!!!!!!!!!! SIMILARITY SCORES: ', similarities)\n    \n    # Filter results based on similarity threshold\n    mask = similarities[0] > similarity_threshold\n    filtered_indices = indices[0][mask]\n    \n    # Return similar examples\n    return examples_df.iloc[filtered_indices]\n\ndef create_prompt(problem, similar_examples_df=None):\n    system_content  = \"\"\"You are a very clever math expert who excels at solving complex mathematical problems through careful step-by-step reasoning. When solving problems, you:\n1. First read and understand the problem carefully [Think about the key information provided]\n2. List all the given information and what needs to be found [Clearly state what we're solving for]\n3. Break down the problem into smaller parts [List the steps needed to solve]\n4. Solve each part systematically [Show each calculation step]\n5. Double-check your work [Verify the solution makes sense]\n6. Provide the final answer as an integer [You MUST put the final answer between \\\\boxed{}]\n\"\"\"\n    \n    if similar_examples_df is not None and not similar_examples_df.empty:\n        system_content += \"\"\"IMPORTANT: Here are some similar problems I've solved before. \n        In case you deem it necessary, you can use them as a reference for your approach to solving the given problem.\\n\\n\"\"\"\n        \n        for _, row in similar_examples_df.iterrows():\n            system_content += f\"Problem: {row['problem']}\\nReasoning: {row['reasoning']}\\nAnswer: {row['answer']}\\n\\n\"\n        \n    user_content = \"Now solve this problem step by step:  \" + problem.tolist()[0]\n    \n    messages = [\n        {\"role\": \"system\", \"content\": system_content},\n        {\"role\": \"user\", \"content\": user_content}\n    ]\n\n    print('!!!!!!!!!! MESSAGGI: ', messages)\n    return messages\n\ndef extract_answer(response_text: str) -> int:\n    # Cerca tutti i numeri all'interno della pattern \\boxed{}\n    matches = re.findall(r'\\\\boxed\\s*\\{(\\d+)\\}', response_text)\n    \n    if not matches:\n        return 0\n    \n    # Prende l'ultimo numero trovato\n    number = int(matches[-1])\n    \n    # Se il numero è maggiore di 1000, restituisce il valore modulo 1000\n    return number % 1000 if number > 1000 else number\n\ndef solve_problem(examples_df: pd.DataFrame, problem: str) -> int:\n    # Load model and tokenizer\n    model, tokenizer = load_model()\n\n    # Find similar examples\n    similar_examples = find_similar_examples(problem, examples_df)\n    \n    # Create prompt with similar examples only\n    messages  = create_prompt(problem, similar_examples)\n\n    chat_input = tokenizer.apply_chat_template(\n        messages, \n        tokenize=True, \n        return_tensors=\"pt\"\n    ).to('cuda')\n    \n    # Generate response\n    outputs = model.generate(\n        chat_input,\n        max_new_tokens=1024 * TOP_K,\n        temperature=1e-3,\n        repetition_penalty=10.,\n        num_return_sequences=1,\n        do_sample=True,\n        pad_token_id=tokenizer.eos_token_id,\n    )\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print('!!!!!!!!!! LA RISPOSTA è: ', response)\n    \n    # Extract answer\n    solution = extract_answer(response)\n    print('!!!!!!!!!! LA SOLUZIONE è: ', solution)\n    \n    return solution","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Replace this function with your inference code.\n# The function should return a single integer between 0 and 999, inclusive.\n# Each prediction (except the very first) must be returned within 30 minutes of the question being provided.\ndef predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n    id_ = id_.item(0)\n    print(\"------\")\n    print(id_)\n    \n    question = question.item(0)\n    answer = solve_problem(examples_df, question)\n    print(question)\n    print(\"------\\n\\n\\n\")\n    return pl.DataFrame({'id': id_, 'answer': answer})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nif __name__ == '__main__':\n    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n        inference_server.AIMO2InferenceServer(predict).serve()\n    else:\n        print('Skipping run for now')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##################### DEBUG #####################\nquestion = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-progress-prize-2/reference.csv').tail(1)['problem']\nsolve_problem(examples_df, question)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TODO: 1) Riordinare i file csv 2) Utilizzare modello Instruct e relativa struttura di chat","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}